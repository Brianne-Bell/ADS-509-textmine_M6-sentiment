{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Sentiment Assignment\n",
    "\n",
    "This notebook holds the Sentiment Assignment for Module 6 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In a previous assignment you put together Twitter data and lyrics data on two artists. In this assignment we apply sentiment analysis to those data sets. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Blackboard. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\zfreitas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Add any additional import statements you need here\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('opinion_lexicon')\n",
    "\n",
    "\n",
    "# Add additional helper functions\n",
    "\n",
    "# Some punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "# Just in case I need something like this.\n",
    " \n",
    "def remove_punctuation(text, punct_set = tw_punct) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) : \n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That \n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "    \n",
    "    # modify this function to return tokens\n",
    "    return text.lower().strip().split() \n",
    "\n",
    "\n",
    "def remove_stop(tokens) :\n",
    "    # modify this function to remove stopwords\n",
    "    return([t for t in tokens if t.lower() not in sw])\n",
    "\n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    \n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)\n",
    "        \n",
    "    return(tokens)\n",
    "\n",
    "\n",
    "def lowercase(text) : \n",
    "    \"\"\" convert text to lowercase and trim excess spacing. \"\"\"\n",
    "    return text.lower().strip()\n",
    "\n",
    "# # apply the `pipeline` techniques from BTAP Ch 1 or 5\n",
    "\n",
    "# my_pipeline = [str.lower, remove_punctuation, tokenize, remove_stop]\n",
    "\n",
    "# lyrics_data[\"tokens\"] = lyrics_data[\"lyrics\"].apply(prepare,pipeline=my_pipeline)\n",
    "# lyrics_data[\"num_tokens\"] = lyrics_data[\"tokens\"].map(len) \n",
    "\n",
    "\n",
    "# twitter_data[\"tokens\"] = twitter_data[\"description\"].apply(prepare,pipeline=my_pipeline)\n",
    "# twitter_data[\"num_tokens\"] = twitter_data[\"tokens\"].map(len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"C:/Users/zfreitas/Dropbox/Classes/USD/ADS-509-01-SP23 - Applied Text Mining/git/\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\"\n",
    "\n",
    "positive_words_file = \"positive-words.txt\"\n",
    "negative_words_file = \"negative-words.txt\"\n",
    "tidy_text_file = \"tidytext_sentiments.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A Pandas data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37d70801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"88 Degrees\"</td>\n",
       "      <td>cher</td>\n",
       "      <td>Stuck in L.A., ain't got no friends \\nAnd so H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"A Different Kind Of Love Song\"</td>\n",
       "      <td>cher</td>\n",
       "      <td>What if the world was crazy and I was sane\\nWo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"After All\"</td>\n",
       "      <td>cher</td>\n",
       "      <td>Well, here we are again\\nI guess it must be fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Again\"</td>\n",
       "      <td>cher</td>\n",
       "      <td>Again evening finds me at your door \\nHere to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Alfie\"</td>\n",
       "      <td>cher</td>\n",
       "      <td>What's it all about, Alfie?\\nIs it just for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>\"We Dance To The Beat\"</td>\n",
       "      <td>robyn</td>\n",
       "      <td>We dance to the beat\\nWe dance to the beat\\nWe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>\"Where Did Our Love Go\"</td>\n",
       "      <td>robyn</td>\n",
       "      <td>Thoughts about you and me \\nThinkin' about wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>\"Who's That Girl\"</td>\n",
       "      <td>robyn</td>\n",
       "      <td>Good girls are pretty like all the time\\nI'm j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>\"With Every Heartbeat\"</td>\n",
       "      <td>robyn</td>\n",
       "      <td>Maybe we could make it all right\\nWe could mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>\"You've Got That Something\"</td>\n",
       "      <td>robyn</td>\n",
       "      <td>Look at me here I am\\nI'm givin all of my lovi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>719 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title artist  \\\n",
       "0                       \"88 Degrees\"   cher   \n",
       "1    \"A Different Kind Of Love Song\"   cher   \n",
       "2                        \"After All\"   cher   \n",
       "3                            \"Again\"   cher   \n",
       "4                            \"Alfie\"   cher   \n",
       "..                               ...    ...   \n",
       "714           \"We Dance To The Beat\"  robyn   \n",
       "715          \"Where Did Our Love Go\"  robyn   \n",
       "716                \"Who's That Girl\"  robyn   \n",
       "717           \"With Every Heartbeat\"  robyn   \n",
       "718      \"You've Got That Something\"  robyn   \n",
       "\n",
       "                                                lyrics  \n",
       "0    Stuck in L.A., ain't got no friends \\nAnd so H...  \n",
       "1    What if the world was crazy and I was sane\\nWo...  \n",
       "2    Well, here we are again\\nI guess it must be fa...  \n",
       "3    Again evening finds me at your door \\nHere to ...  \n",
       "4    What's it all about, Alfie?\\nIs it just for th...  \n",
       "..                                                 ...  \n",
       "714  We dance to the beat\\nWe dance to the beat\\nWe...  \n",
       "715  Thoughts about you and me \\nThinkin' about wha...  \n",
       "716  Good girls are pretty like all the time\\nI'm j...  \n",
       "717  Maybe we could make it all right\\nWe could mak...  \n",
       "718  Look at me here I am\\nI'm givin all of my lovi...  \n",
       "\n",
       "[719 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the lyrics data\n",
    "lyrics_df = pd.read_pickle(data_location + \"lyrics_df.pkl\")\n",
    "lyrics_df = lyrics_df[[\"title\", \"artist\", \"lyrics\"]]\n",
    "lyrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "debcac5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cher</td>\n",
       "      <td>ùôøùöõùöòùöûùöç ùöúùöûùöôùöôùöòùöõùöùùöéùöõ ùöòùöè ùöñùöéùöúùöúùö¢ ùöãùöûùöóùöú &amp; ùöïùöéùöêùöêùöíùöóùöêùöú</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher</td>\n",
       "      <td>163„éùÔºèÊÑõ„Åã„Å£„Å∑üíú26Ê≠≥üçí Â∑•„ÄáÂ•Ω„Åç„Å™Â•≥„ÅÆÂ≠êüíì „Éï„Ç©„É≠„Éº„Åó„Å¶„Åè„Çå„Åü„ÇâDM„Åó„Åæ„Åôüß°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cher</td>\n",
       "      <td>csu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cher</td>\n",
       "      <td>Writer @Washinformer @SpelmanCollege alumna #D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184476</th>\n",
       "      <td>robyn</td>\n",
       "      <td>singer of songs, type 1 diabetic, tired $jakel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184477</th>\n",
       "      <td>robyn</td>\n",
       "      <td>Dadx2/ Con-Arch/ Photographer/ DK #stemgr√∏nnes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184478</th>\n",
       "      <td>robyn</td>\n",
       "      <td>A year to change a life is still a year ‚ú®üòå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184479</th>\n",
       "      <td>robyn</td>\n",
       "      <td>Head of Consumer - Mango. Made in Melbourne. R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184480</th>\n",
       "      <td>robyn</td>\n",
       "      <td>Stand for what is right, even if you stand alone.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8184481 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        artist                                        description\n",
       "0         cher                                                   \n",
       "1         cher           ùôøùöõùöòùöûùöç ùöúùöûùöôùöôùöòùöõùöùùöéùöõ ùöòùöè ùöñùöéùöúùöúùö¢ ùöãùöûùöóùöú & ùöïùöéùöêùöêùöíùöóùöêùöú\n",
       "2         cher          163„éùÔºèÊÑõ„Åã„Å£„Å∑üíú26Ê≠≥üçí Â∑•„ÄáÂ•Ω„Åç„Å™Â•≥„ÅÆÂ≠êüíì „Éï„Ç©„É≠„Éº„Åó„Å¶„Åè„Çå„Åü„ÇâDM„Åó„Åæ„Åôüß°\n",
       "3         cher                                                csu\n",
       "4         cher  Writer @Washinformer @SpelmanCollege alumna #D...\n",
       "...        ...                                                ...\n",
       "8184476  robyn  singer of songs, type 1 diabetic, tired $jakel...\n",
       "8184477  robyn  Dadx2/ Con-Arch/ Photographer/ DK #stemgr√∏nnes...\n",
       "8184478  robyn         A year to change a life is still a year ‚ú®üòå\n",
       "8184479  robyn  Head of Consumer - Mango. Made in Melbourne. R...\n",
       "8184480  robyn  Stand for what is right, even if you stand alone.\n",
       "\n",
       "[8184481 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the twitter data\n",
    "twitter_df = pd.read_pickle(data_location + \"twitter_df.pkl\")\n",
    "twitter_df = twitter_df[[\"artist\", \"description\"]]\n",
    "twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ace520d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of lists of the description with the key being the artist.\n",
    "twitter_dict = { \"cher\": list(twitter_df[twitter_df[\"artist\"]=='cher'][\"description\"]),\n",
    "\"robyn\": list(twitter_df[twitter_df[\"artist\"]=='robyn'][\"description\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the positive and negative words and the\n",
    "# tidytext sentiment. Store these so that the positive\n",
    "# words are associated with a score of +1 and negative words\n",
    "# are associated with a score of -1. You can use a dataframe or a \n",
    "# dictionary for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5ea490c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>lexicon</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-faced</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-faces</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abnormal</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abolish</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abominable</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4778</th>\n",
       "      <td>zaps</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4779</th>\n",
       "      <td>zealot</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4780</th>\n",
       "      <td>zealous</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4781</th>\n",
       "      <td>zealously</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4782</th>\n",
       "      <td>zombie</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4783 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word sentiment lexicon  score\n",
       "0        2-faced  negative             -1\n",
       "1        2-faces  negative             -1\n",
       "2       abnormal  negative             -1\n",
       "3        abolish  negative             -1\n",
       "4     abominable  negative             -1\n",
       "...          ...       ...     ...    ...\n",
       "4778        zaps  negative             -1\n",
       "4779      zealot  negative             -1\n",
       "4780     zealous  negative             -1\n",
       "4781   zealously  negative             -1\n",
       "4782      zombie  negative             -1\n",
       "\n",
       "[4783 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Negative Words\n",
    "neg_words = pd.read_csv(\"negative-words.txt\",names=['word'], skiprows=34, encoding = \"ISO-8859-1\")\n",
    "neg_words[\"sentiment\"] = \"negative\"\n",
    "neg_words[\"lexicon\"] = \"\"\n",
    "neg_words[\"score\"] = -1\n",
    "neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd82e7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>lexicon</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a+</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abound</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abounds</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abundance</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abundant</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>youthful</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>zeal</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>zenith</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>zest</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>zippy</td>\n",
       "      <td>positive</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2006 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word sentiment lexicon  score\n",
       "0            a+  positive              1\n",
       "1        abound  positive              1\n",
       "2       abounds  positive              1\n",
       "3     abundance  positive              1\n",
       "4      abundant  positive              1\n",
       "...         ...       ...     ...    ...\n",
       "2001   youthful  positive              1\n",
       "2002       zeal  positive              1\n",
       "2003     zenith  positive              1\n",
       "2004       zest  positive              1\n",
       "2005      zippy  positive              1\n",
       "\n",
       "[2006 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Positive Words\n",
    "pos_words = pd.read_csv(\"positive-words.txt\",names=['word'], skiprows=34, encoding = \"ISO-8859-1\")\n",
    "pos_words[\"sentiment\"] = \"positive\"\n",
    "pos_words[\"lexicon\"] = \"\"\n",
    "pos_words[\"score\"] = 1\n",
    "pos_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e156718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>lexicon</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>negative</td>\n",
       "      <td>nrc</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>negative</td>\n",
       "      <td>nrc</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>negative</td>\n",
       "      <td>nrc</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abba</td>\n",
       "      <td>positive</td>\n",
       "      <td>nrc</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abduction</td>\n",
       "      <td>negative</td>\n",
       "      <td>nrc</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15128</th>\n",
       "      <td>win</td>\n",
       "      <td>positive</td>\n",
       "      <td>loughran</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15129</th>\n",
       "      <td>winner</td>\n",
       "      <td>positive</td>\n",
       "      <td>loughran</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15130</th>\n",
       "      <td>winners</td>\n",
       "      <td>positive</td>\n",
       "      <td>loughran</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15131</th>\n",
       "      <td>winning</td>\n",
       "      <td>positive</td>\n",
       "      <td>loughran</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15132</th>\n",
       "      <td>worthy</td>\n",
       "      <td>positive</td>\n",
       "      <td>loughran</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15133 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word sentiment   lexicon  score\n",
       "0          abandon  negative       nrc   -1.0\n",
       "1        abandoned  negative       nrc   -1.0\n",
       "2      abandonment  negative       nrc   -1.0\n",
       "3             abba  positive       nrc    1.0\n",
       "4        abduction  negative       nrc   -1.0\n",
       "...            ...       ...       ...    ...\n",
       "15128          win  positive  loughran    1.0\n",
       "15129       winner  positive  loughran    1.0\n",
       "15130      winners  positive  loughran    1.0\n",
       "15131      winning  positive  loughran    1.0\n",
       "15132       worthy  positive  loughran    1.0\n",
       "\n",
       "[15133 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get tidytext words\n",
    "tidytext = pd.read_csv(\"tidytext_sentiments.txt\", sep = \"\\t\")\n",
    "tidytext.loc[tidytext[\"sentiment\"].isin([\"positive\"]), 'score'] = 1\n",
    "tidytext.loc[tidytext[\"sentiment\"].isin([\"negative\"]), 'score'] = -1\n",
    "tidytext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0e5ed36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>lexicon</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>negative</td>\n",
       "      <td>nrc</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>negative</td>\n",
       "      <td>nrc</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>negative</td>\n",
       "      <td>nrc</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abba</td>\n",
       "      <td>positive</td>\n",
       "      <td>nrc</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abduction</td>\n",
       "      <td>negative</td>\n",
       "      <td>nrc</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21917</th>\n",
       "      <td>zaps</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21918</th>\n",
       "      <td>zealot</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21919</th>\n",
       "      <td>zealous</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21920</th>\n",
       "      <td>zealously</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21921</th>\n",
       "      <td>zombie</td>\n",
       "      <td>negative</td>\n",
       "      <td></td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21922 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word sentiment lexicon  score\n",
       "0          abandon  negative     nrc   -1.0\n",
       "1        abandoned  negative     nrc   -1.0\n",
       "2      abandonment  negative     nrc   -1.0\n",
       "3             abba  positive     nrc    1.0\n",
       "4        abduction  negative     nrc   -1.0\n",
       "...            ...       ...     ...    ...\n",
       "21917         zaps  negative           -1.0\n",
       "21918       zealot  negative           -1.0\n",
       "21919      zealous  negative           -1.0\n",
       "21920    zealously  negative           -1.0\n",
       "21921       zombie  negative           -1.0\n",
       "\n",
       "[21922 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine words into one dataframe.\n",
    "word_score = pd.concat([tidytext, pos_words, neg_words], ignore_index=True)\n",
    "word_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Songs\n",
    "\n",
    "In this section, score the sentiment for all the songs for both artists in your data set. Score the sentiment by manually calculating the sentiment using the combined lexicons provided in this repository. \n",
    "\n",
    "After you have calculated these sentiments, answer the questions at the end of this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "664f8d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>Bing_Liu_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"The Long And Winding Road\"</td>\n",
       "      <td>The long and winding road \\nThat leads to your...</td>\n",
       "      <td>0.037975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"Save The Children\"</td>\n",
       "      <td>Don't look over my shoulder, I'm trying to rea...</td>\n",
       "      <td>-0.017045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"Dark Lady\"</td>\n",
       "      <td>The fortune queen of New Orleans\\nWas brushing...</td>\n",
       "      <td>-0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"Lovers Forever\"</td>\n",
       "      <td>Imagine a life without death or disease\\nThe k...</td>\n",
       "      <td>0.042254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"The Thought Of Loving You\"</td>\n",
       "      <td>The thought of loving you\\nThe way I'm longing...</td>\n",
       "      <td>0.065476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"Behind The Door\"</td>\n",
       "      <td>Behind the door of every house,\\nIn every stre...</td>\n",
       "      <td>0.016760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"Rudy\"</td>\n",
       "      <td>Rudy toot, it's been a long time\\nSince we sai...</td>\n",
       "      <td>-0.018519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Jack U Off\"</td>\n",
       "      <td>If you're looking for somewhere to go\\nThought...</td>\n",
       "      <td>0.016835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"The Shoop Shoop Song (It's In His Kiss)\"</td>\n",
       "      <td>Does he love me I want to know\\nHow can I tell...</td>\n",
       "      <td>0.066879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"Heart Of Stone\"</td>\n",
       "      <td>Beneath the white fire of the moon \\nLove's wi...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist                                      title  \\\n",
       "565   cher                \"The Long And Winding Road\"   \n",
       "210   cher                        \"Save The Children\"   \n",
       "46    cher                                \"Dark Lady\"   \n",
       "163   cher                           \"Lovers Forever\"   \n",
       "576   cher                \"The Thought Of Loving You\"   \n",
       "18    cher                          \"Behind The Door\"   \n",
       "520   cher                                     \"Rudy\"   \n",
       "677  robyn                               \"Jack U Off\"   \n",
       "574   cher  \"The Shoop Shoop Song (It's In His Kiss)\"   \n",
       "402   cher                           \"Heart Of Stone\"   \n",
       "\n",
       "                                                lyrics  Bing_Liu_Score  \n",
       "565  The long and winding road \\nThat leads to your...        0.037975  \n",
       "210  Don't look over my shoulder, I'm trying to rea...       -0.017045  \n",
       "46   The fortune queen of New Orleans\\nWas brushing...       -0.025641  \n",
       "163  Imagine a life without death or disease\\nThe k...        0.042254  \n",
       "576  The thought of loving you\\nThe way I'm longing...        0.065476  \n",
       "18   Behind the door of every house,\\nIn every stre...        0.016760  \n",
       "520  Rudy toot, it's been a long time\\nSince we sai...       -0.018519  \n",
       "677  If you're looking for somewhere to go\\nThought...        0.016835  \n",
       "574  Does he love me I want to know\\nHow can I tell...        0.066879  \n",
       "402  Beneath the white fire of the moon \\nLove's wi...        0.000000  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "lyrics_sentiment_df = lyrics_df.copy() \n",
    "\n",
    "pos_score = 1\n",
    "neg_score = -1\n",
    "word_dict = word_score.set_index('word').to_dict()[\"score\"]\n",
    "\n",
    "def bing_liu_score(text):\n",
    "    sentiment_score = 0\n",
    "    bag_of_words = word_tokenize(text.lower())\n",
    "    for word in bag_of_words:\n",
    "        if word in word_dict:\n",
    "            sentiment_score += word_dict[word]\n",
    "    return (sentiment_score / len(bag_of_words)) if len(bag_of_words) else 0\n",
    "\n",
    "lyrics_sentiment_df['Bing_Liu_Score'] = lyrics_sentiment_df['lyrics'].apply(bing_liu_score)\n",
    "lyrics_sentiment_df[['artist','title','lyrics','Bing_Liu_Score']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8334f4",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "Q: Overall, which artist has the higher average sentiment per song? \n",
    "\n",
    "A: <!-- Your answer here -->\n",
    "\n",
    "---\n",
    "\n",
    "Q: For your first artist, what are the three songs that have the highest and lowest sentiments? Print the lyrics of those songs to the screen. What do you think is driving the sentiment score? \n",
    "\n",
    "A: <!-- Your answer here -->\n",
    "\n",
    "---\n",
    "\n",
    "Q: For your second artist, what are the three songs that have the highest and lowest sentiments? Print the lyrics of those songs to the screen. What do you think is driving the sentiment score? \n",
    "\n",
    "A: <!-- Your answer here -->\n",
    "\n",
    "---\n",
    "\n",
    "Q: Plot the distributions of the sentiment scores for both artists. You can use `seaborn` to plot densities or plot histograms in matplotlib.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe644d",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Twitter Descriptions\n",
    "\n",
    "In this section, define two sets of emojis you designate as positive and negative. Make sure to have at least 10 emojis per set. You can learn about the most popular emojis on Twitter at [the emojitracker](https://emojitracker.com/). \n",
    "\n",
    "Associate your positive emojis with a score of +1, negative with -1. Score the average sentiment of your two artists based on the Twitter descriptions of their followers. The average sentiment can just be the total score divided by number of followers. You do not need to calculate sentiment on non-emoji content for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a5c1d25",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m             sentiment_score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m word_dict[word]\n\u001b[0;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m (sentiment_score \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(bag_of_words)) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(bag_of_words) \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 17\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mBing_Liu_Score\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mdescription\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(bing_liu_score)\n\u001b[0;32m     18\u001b[0m df[[\u001b[39m'\u001b[39m\u001b[39martist\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mBing_Liu_Score\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39msample(\u001b[39m10\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[51], line 11\u001b[0m, in \u001b[0;36mbing_liu_score\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbing_liu_score\u001b[39m(text):\n\u001b[0;32m     10\u001b[0m     sentiment_score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 11\u001b[0m     bag_of_words \u001b[39m=\u001b[39m word_tokenize(text\u001b[39m.\u001b[39;49mlower())\n\u001b[0;32m     12\u001b[0m     \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m bag_of_words:\n\u001b[0;32m     13\u001b[0m         \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m word_dict:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\nltk\\tokenize\\__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m     token \u001b[39mfor\u001b[39;49;00m sent \u001b[39min\u001b[39;49;00m sentences \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m _treebank_word_tokenizer\u001b[39m.\u001b[39;49mtokenize(sent)\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\nltk\\tokenize\\__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m--> 131\u001b[0m     token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39;49mtokenize(sent)\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\nltk\\tokenize\\destructive.py:160\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    157\u001b[0m     text \u001b[39m=\u001b[39m regexp\u001b[39m.\u001b[39msub(substitution, text)\n\u001b[0;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m regexp, substitution \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPUNCTUATION:\n\u001b[1;32m--> 160\u001b[0m     text \u001b[39m=\u001b[39m regexp\u001b[39m.\u001b[39msub(substitution, text)\n\u001b[0;32m    162\u001b[0m \u001b[39m# Handles parentheses.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m regexp, substitution \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPARENS_BRACKETS\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "df = twitter_df.copy() # [[\"description\"]]\n",
    "\n",
    "pos_score = 1\n",
    "neg_score = -1\n",
    "word_dict = word_score.set_index('word').to_dict()[\"score\"]\n",
    "\n",
    "def bing_liu_score(text):\n",
    "    sentiment_score = 0\n",
    "    bag_of_words = word_tokenize(text.lower())\n",
    "    for word in bag_of_words:\n",
    "        if word in word_dict:\n",
    "            sentiment_score += word_dict[word]\n",
    "    return (sentiment_score / len(bag_of_words)) if len(bag_of_words) else 0\n",
    "\n",
    "df['Bing_Liu_Score'] = df['description'].apply(bing_liu_score)\n",
    "df[['artist','description','Bing_Liu_Score']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb92eb93",
   "metadata": {},
   "source": [
    "Q: What is the average sentiment of your two artists? \n",
    "\n",
    "A: <!-- Your answer here --> \n",
    "\n",
    "---\n",
    "\n",
    "Q: Which positive emoji is the most popular for each artist? Which negative emoji? \n",
    "\n",
    "A: <!-- Your answer here --> \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b33cfd37f5195bd7836c1451c6eaacc84fbbad3c54541ec8bad2790bfb3f777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
